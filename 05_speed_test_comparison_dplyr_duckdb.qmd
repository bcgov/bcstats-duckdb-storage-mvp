---
title: "DuckDB Evaluation"
date: today
format: html
toc: true
params:
  n_trials: 5
  n_max: !r Inf
  run_loop: no
editor_options: 
  chunk_output_type: console
---

```{r setup}
pacman::p_load(tictoc, tidyverse, duckdb, zeallot, pryr, readr)

csv_path = "G:/Operations/Data Science and Analytics/2024_bcstats_db/csvs"

csv_names = c(
  "98-401-X2021006_English_CSV_data_BritishColumbia-utf8.csv",
  "BC Stat Population Estimates_20240429.csv",
  "CLR_EXT_20231227.csv",
  "bca_folio_gnrl_property_values_20240825_WEEKLY.csv",
  "bca_folio_sales_20240825_WEEKLY.csv"
)
```

# Introductions

The purpose of this project is to provide a high-level evaluation of [DuckDB](https://duckdb.org) as a solution for projects involving databases at BC Stats.

# Advantages of DuckDB

Section left blank for Jon.

# Disadvantages of DuckDB

Section left blank for Jon.

# Loading Data into DuckDB

Section left blank for Jon. Note you can use bash scripts here -- maybe useful for embedding the dbt code?

```{bash}
# echo hello jon!!!
```

# Speed Tests

## Introduction

In this section, we compare the speed of certain operations between `DuckDB`, `duckplyr` and `dplyr` in R. The operations are:

-   loading .csv files as standalone R data frames or into a DuckDB database
-   querying either R data frames using `dbplyr` or `duckplyr` syntax, or querying DuckDB databases.

We will use the [`tictoc`](https://cran.r-project.org/web/packages/tictoc/index.html) package to get speed timestamps.

Note that we did not load `duckplyr` directly in order to avoid overwriting `dplyr` methods. See the [duckplyr website](https://duckplyr.tidyverse.org/index.html) for more information.

We will test speeds on the following `csv` files stored on the LAN.

```{r}
csvs = fs::dir_info(csv_path, regexp = "\\.csv$") |> 
  select(path, size) |>
  mutate(csv = fs::path_file(path), .before=1) |>
  filter(csv %in% csv_names) |>
  mutate(csv = factor(csv, levels = csv_names)) |>
  arrange(csv) |>
  mutate(csv = as.character(csv)) |>
  mutate(csv_id = row_number(), .before=1)

csvs
```

## Issues

### Issue With Query Consistency

csvs/CLR_EXT_20231227.csv

birth_yr_mon can't use ym() in DuckDB

### Issue Reading some `csv` Files

Some `csv` files throw errors when being read with DuckDB. For example, in this section, `duckdb::duckdb_read_csv` throws an error using its default values on a particular `csv` file (because it incorrectly reads the `SYMBOL` column as a Boolean.

## Speed Tests

In this section, we run the main speed tests. We will compare the speeds of `dplyr`, `duckplyr` and `DuckDB`. For load times, we will compare `readr::read_csv` with `duckdb::read_csv_duckdb`. Also, each `csv` will be read and have a basic operation performed on it (equivalent to a SQL select query)---these operations are specific to each file, are entirely made up, and are saved as character vectors in a list.

```{r}
queries = list(
  r = list(),
  duckDB = list()
)

queries$r$dplyr = c(
   "df |>
    mutate(C1_COUNT_TOTAL = as.double(C1_COUNT_TOTAL)) |>
    group_by(CENSUS_YEAR, GEO_LEVEL, GEO_NAME) |>
    summarise(C1_COUNT_TOTAL = sum(C1_COUNT_TOTAL), .groups='drop') |>
    arrange(CENSUS_YEAR, GEO_LEVEL, GEO_NAME)",
  "df |>
    count(CITY, SEX) |>
    arrange(CITY, SEX)",
  "df |>
    count(CITY) |>
    arrange(CITY)",
  "df |>
    summarise(across(everything(), min))",
  "df |>
    count(JURISDICTION, CONVEYANCE_TYPE_DESCRIPTION) |>
    arrange(JURISDICTION, CONVEYANCE_TYPE_DESCRIPTION)"
  )
```

The other two vectors of queries are equivalents of `csvs$expr_dplyr`:

```{r}
queries$r$duckplyr = map_chr(queries$r$dplyr, ~str_replace(., "df |>\n", "df |>\n    duckplyr::as_duckplyr_tibble() "))

queries$duckDB$duckDB = map_chr(queries$r$dplyr, ~str_replace(., "df |>", 'tbl(db, "t1") ')) |> paste(" |>\n  collect()")

# queries = map(queries, function(q) map(q, ~parse(text = .))) |>
#   as_tibble() |>
#   mutate(query_id = row_number(), .before=1)

#stopifnot(nrow(queries) == nrow(csvs))

stopifnot(all(map(flatten(queries), length) == nrow(csvs)))
```

The following code blocks define some helper functions and then run all the operations and saves the time required for each.

```{r}
# run a function with parameters and return both the function's output as well as the time of running it
time_function = function(fn, params) {
  tic()
  out = do.call(fn, params)
  toc(log = T, quiet = T)
  time = as.double(word(tail(tic.log(), 1, 1)))
  list(object = out, time = time)
}

# the function to read a csv using readr::read_csv
read_r = function(file, quote = "\"") time_function(fn = readr::read_csv, params = list(file = file, quote = quote))

# the function to read a csv using readr::read_csv. It tries to read the csv using the default nrow.check value; if that fails, it increases nrow.check to params$n_max
read_duckDB = function(name = "t1", files, delim = ',', header = T, transaction = T) {
    tryCatch({
      output = time_function(duckdb::duckdb_read_csv, params = list(conn = db, name = name, files = files, delim = delim, header = header, transaction = transaction))
      return(list(db_return_code = output$object, time = output$time, failed = F))
    }, error = function(e) {
      output = time_function(duckdb::duckdb_read_csv, params = list(conn = db, name = name, files = files, delim = delim, header = header, transaction = transaction, nrow.check = params$n_max))
    return(list(db_return_code = output$object, time = output$time, failed = T))
    }
  )
}

# create a tibble that organizes the trials
trials = csvs |>
  select(csv_id) |>
  crossing(trial_id = 1:params$n_trials) |>
  arrange(trial_id, csv_id)
```

```{r}
#| eval: !expr params$run_loop

results = tibble()

for (i in 1:nrow(trials)) {
  
  # extract the correct rows for the csvs and queries
  csvs_i = filter(csvs, csv_id == trials[[i, 'csv_id']])
  queries_i_r = lapply(queries$r, function(x) parse(text = x[[trials$csv_id[i]]]))
  queries_i_duckDB = lapply(queries$duckDB, function(x) parse(text = x[[trials$csv_id[i]]]))
  
  message("\n\niteration: ", i, " of ", nrow(trials), "\ncsv: ", csvs_i$csv, "\ntrial: ", trials$trial_id[i], " of ", params$n_trials, "\n\n")
  
  # load df into R
  message("loading R")
  c(df, load_time_df) %<-% read_r(csvs_i$path)
  
  # load duckDB
  message("loading duckDB")
  db = duckdb::dbConnect(duckdb::duckdb())
  c(db_return_code, load_time_duckDB, duckDB_failed_first_try) %<-% read_duckDB(files = csvs_i$path)
  
  # run queries on the R dataframe
  message("running queries on R")
  queries_r = map(queries_i_r, ~time_function(eval, list(.)))
  
  # run queries on the duckDB db
  message("running queries on duckDB")
  queries_duckDB = map(queries_i_duckDB, ~time_function(eval, list(.)))
  
  size_df = object_size(df)
  size_duckDB = object_size(db)
  
  results_i = tibble_row(
    trials[i, ],
    
    load_time_df = load_time_df, 
    load_time_duckDB = load_time_duckDB,
    duckDB_failed_first_try = duckDB_failed_first_try,
    
    eval_time_dplyr = queries_r$query_dplyr$time,
    eval_time_duckplyr = queries_r$query_duckplyr$time,
    eval_time_duckDB = queries_duckDB$query_duckDB$time,
    
    result_dplyr = list(queries_r$query_dplyr$object), 
    result_duckplyr = list(queries_r$query_duckplyr$object), 
    result_duckDB = list(queries_duckDB$query_duckDB$object), 
    
    size_df = list(size_df), 
    size_duckDB = list(size_duckDB)
  )
  
  results = bind_rows(results, results_i)
  
  dbDisconnect(db)
}

if (!fs::dir_exists("RDS")) fs::dir_create("RDS")
saveRDS(results, "RDS/results.Rds")
```


## Results

In this section, we examine the results. First, we check whether the results of the queries were consistent.

```{r}
results = readRDS("RDS/results.Rds") |>
  mutate(across(matches("time"),  ~./60)) # convet time to minutes
  
length_check = results |>
  inner_join(csvs) |>
  arrange(path, trial_id) |>
  select(starts_with("result")) |>
  apply(2, unique)
  
# this should be TRUE: it checks whether the df's returned from all the trials are identical
all(map_int(length_check, length) == nrow(csvs)) 

map_int(1:nrow(csvs), function(i) {
  dfs = list()
  for (df in length_check) {
    dfs = append(dfs, df[i])
  }
  length(unique(dfs))
})

# This should be a vector of 1s of length `nrow(csvs)`. It goes line by line and, for each  each csv, returns the number of identical dataframes for each loading procedure. Any number greater than one indicates that there are differences in the loaded dataframes, which is not ideal.
```

An informal inspection of these dataframes indicates that the results are **mostly** the same. The differences are mostly due to situations in which, for example, a value is coded as `NA` in one dataframe and 0 in the other.

This next table gives the mean time for all the events.

```{r}
# means
results |>
  inner_join(csvs) |>
  select(csv, size, matches("time")) |>
  mutate(size = as.character(size)) |>
  group_by(csv, size) |>
  summarise(across(matches("time"), mean)) |>
  mutate(across(matches("time"), ~round(., 1))) |>
  reactable::reactable()
```

This graph shows the load times by file.

```{r}
results |>
  select(csv_id, trial_id, load_time_df, load_time_duckDB) |>
  mutate(trial_id = as_factor(trial_id)) |>
  pivot_longer(cols = 3:4) |>
  ggplot(aes(fill=name, y=value, x=trial_id)) +
  facet_wrap(~csv_id, scales = 'free_y') +
  geom_col(position = 'dodge') +
  ggthemes::theme_clean() +
  theme(legend.position = 'bottom') +
  scale_fill_viridis_d(option = "D") +
  labs(fill=NULL, y="time (min)")
```

This graph gives the load time versus size scatter plot.

```{r}
results |>
  inner_join(csvs) |>
  mutate(csv_id = as_factor(csv_id)) |>
  select(csv_id, size, load_time_df, load_time_duckDB) |>
  pivot_longer(cols = 3:4) |>
  ggplot(aes(x=size, y=value, color=name, shape = csv_id)) +
  geom_jitter(size=3) +
  ggthemes::theme_clean() +
  theme(legend.position = 'bottom') +
  labs(color=NULL, x="file size", y="load time (min)") +
  guides(shape = 'none') +
  scale_color_viridis_d(option = "D")
```
