---
title: "DuckDB Evaluation"
date: today
format: html
toc: true
params:
  n_trials: 5
  nrow_check: 500
  nrow_by: !expr 10^6
  run_boolean_problem: no
  run_loop: yes
editor_options: 
  chunk_output_type: console
---

```{r setup}
pacman::p_load(tictoc, tidyverse, duckdb, zeallot, pryr)
#csv_path = "G:/Operations/Data Science and Analytics/2024_bcstats_db/csvs"
csv_path = "RDS"
```

# Introductions

The purpose of this project is to provide a high-level evaluation of [DuckDB](https://duckdb.org) as a solution for projects involving databases at BC Stats.

# Advantages of DuckDB

Section left blank for Jon.

# Disadvantages of DuckDB

Section left blank for Jon.

# Loading Data into DuckDB

Section left blank for Jon. Note you can use bash scripts here -- maybe useful for embedding the dbt code?

```{bash}
echo hello jon!!!
```

# Speed Tests

## Introduction

In this section, we compare the speed of certain operations between `DuckDB`, `duckplyr` and `dplyr` in R. The operations are:

-   loading .csv files as standalone R data frames or into a DuckDB database
-   querying either R data frames using `dbplyr` or `duckplyr` syntax, or querying DuckDB databases.

We will use the [`tictoc`](https://cran.r-project.org/web/packages/tictoc/index.html) package to get speed timestamps.

Note that we did not load `duckplyr` directly in order to avoid overwriting `dplyr` methods. See the [duckplyr website](https://duckplyr.tidyverse.org/index.html) for more information.

We will test speeds on the following `csv` files stored on the LAN.

```{r}
csvs = fs::dir_info(csv_path, regexp = "\\.csv$") |> 
  select(path, size) |>
  mutate(csv = fs::path_file(path), .before=1) |>
  #arrange(desc(size)) |>
  mutate(csv_id = row_number(), .before=1)

csvs
```

## Issues

### Issue With Query Consistency

csvs/CLR_EXT_20231227.csv

birth_yr_mon can't use ym() in DuckDB

### Issue Reading some `csv` Files

Some `csv` files throw errors when being read with DuckDB. For example, in this section, `duckdb::duckdb_read_csv` throws an error using its default values on a particular `csv` file (because it incorrectly reads the `SYMBOL` column as a Boolean.

```{r}
if (params$run_boolean_problem) {

  problematic_csv_path = csvs$path[[1]]
  db_boolean_problem = duckdb::dbConnect(duckdb::duckdb())
  
  # this gives an error because it thinks the "SYMBOL" column is a boolean
  tryCatch(
    duckdb::duckdb_read_csv(db_boolean_problem, name = "test_table", files = problematic_csv_path, header = TRUE, transaction = T),
    error = function(e) print(e)
  )
}
```

The following function presents a quick workaround to this problem, that progressively increases the `nrow.check` parameter whenever `duckdb::duckdb_read_csv` throws an error. The function stops when `duckdb::duckdb_read_csv` succeeds and returns the value of `nrow.check`.

```{r}
adjust_nrow_check = function(conn, name, files, header, transaction, nrow_check, by, quiet=T) {
  i = nrow_check
  while (T) {
    if (!quiet) message("\ni: ", i)
    tryCatch({
      duckdb::duckdb_read_csv(conn = conn, name = name, files =  files, header = header, transaction = transaction, nrow.check = i)
      return(i)
    },
    error = function(e) NULL)
    i = i + by
  }
}
```

Next, we will find a value for `nrow.check` that works, load the `csv` into the duckDB database, and print a count of the problematic column.

```{r}
if (params$run_boolean_problem) {

  i = adjust_nrow_check(conn = db_boolean_problem, name = "test_table", files = problematic_csv_path, header = T, transaction = T, nrow_check = params$nrow_check, by = 500)
  
  cat("The `duckdb::duckdb_read_csv` function works when `nrow.check` ==", i)
  
  symbol_duckdb = tbl(db_boolean_problem, "test_table") |>
    count(SYMBOL) |>
    arrange(n) |>
    collect()
  
  symbol_duckdb
  saveRDS(list(i, symbol_duckdb), "RDS/run_boolean_problem.Rds")
}
```

```{r}
#| echo: false

if (!params$run_boolean_problem) {
  c(i, symbol_duckdb) %<-% readRDS("RDS/run_boolean_problem.Rds")
  
  cat("The `duckdb::duckdb_read_csv` function works when `nrow.check` ==", i)
  symbol_duckdb
}
```

## Speed Tests

In this section, we run the main speed tests. We will compare the speeds of `dplyr`, `duckplyr` and `DuckDB`. For load times, we will compare R's `read.csv2` function with DuckDB's `read_csv_duckdb`. Each `csv` will be read and have a basic operation performed on it (equivalent to a SQL select query)---these operations are specific to each file, are entirely made up, and are saved as characters vectors in the `csvs` dataframe. 

`csvs$exprs_dplyr` contains the queries using `dplyr`.

```{r}
queries = list(
  r = list(),
  duckDB = list()
)

queries$r$query_dplyr = c(
  "df |>
    as_tibble() |>
    mutate(C1_COUNT_TOTAL = as.double(C1_COUNT_TOTAL)) |>
    group_by(CENSUS_YEAR, GEO_LEVEL, GEO_NAME) |>
    summarise(C1_COUNT_TOTAL = sum(C1_COUNT_TOTAL), .groups='drop') |>
    arrange(CENSUS_YEAR, GEO_LEVEL, GEO_NAME)",
  "df |>
    as_tibble() |>
    count(CITY, SEX) |>
    arrange(CITY, SEX)",
  "df |>
    as_tibble() |>
    select(BIRTH_YR_MON) |>
    arrange(BIRTH_YR_MON)",
  "df |>
    as_tibble() |>
    count(JURISDICTION, CONVEYANCE_TYPE_DESCRIPTION) |>
    arrange(JURISDICTION, CONVEYANCE_TYPE_DESCRIPTION)",
  "df |>
    as_tibble() |>
    summarise(across(everything(), min))"
)
```

The other two vectors of queries are equivalents of `csvs$expr_dplyr`:

```{r}
queries$r$query_duckplyr = map_chr(queries$r$query_dplyr, ~str_replace(., "as_tibble", "duckplyr::as_duckplyr_tibble"))

queries$duckDB$query_duckDB = map_chr(queries$r$query_dplyr, ~str_replace(., "df\\s+\\|>\\\n\\s+as\\_tibble\\(\\)\\s+", 'tbl(db, "t1") ')) |> paste(" |>\n  collect()")

# queries = map(queries, function(q) map(q, ~parse(text = .))) |>
#   as_tibble() |>
#   mutate(query_id = row_number(), .before=1)

#stopifnot(nrow(queries) == nrow(csvs))

stopifnot(all(map(flatten(queries), length) == nrow(csvs)))
```

The following code blocks define some helper functions and then run all the operations and saves the time required for each.

```{r}
# run a function on an object and return the function's output as well as the time of running it
time_function = function(fn, params) {
  tic()
  out = do.call(fn, params)
  toc(log = T, quiet = T)
  time = as.double(word(tail(tic.log(), 1, 1)))
  list(object = out, time = time)
}

read_duckDB = function(name = "t1", files, delim = ',', header = T, transaction = T) {
    tryCatch(duckdb::duckdb_read_csv(conn = db, name = "t1", files = csvs_i$path, delim = delim, header = header, transaction = transaction), error = function(e) duckdb::duckdb_read_csv(conn = db, name = "t1", files = csvs_i$path, delim = delim, header = header, transaction = transaction, nrow.check = Inf))
  }
```


```{r}
trials = csvs |>
  select(csv_id) |>
  crossing(trial_id = 1:params$n_trials) |>
  arrange(trial_id, csv_id)

trials
```

```{r}
#| eval: !expr params$run_loop

results = tibble()

for (i in 1:nrow(trials)) {
  csvs_i = filter(csvs, csv_id == trials[[i, 'csv_id']])
  
  queries_i_r = lapply(queries$r, function(x) parse(text = x[[trials$csv_id[i]]]))
  queries_i_duckDB = lapply(queries$duckDB, function(x) parse(text = x[[trials$csv_id[i]]]))
  
  message("\n\niteration: ", i, " of ", nrow(trials), "\ncsv: ", csvs_i$csv, "\ntrial: ", trials$trial_id[i], " of ", params$n_trials, "\n\n")
  
  # load df into R
  c(df, load_time_df) %<-% time_function(fn = read.csv2, params = list(file = csvs_i$path, sep = ";", quote = "\"", na.strings = ""))
  
  # load duckDB
  db = duckdb::dbConnect(duckdb::duckdb())
  c(db_return_code, load_time_duckDB) %<-% time_function(fn = read_duckDB, params = list(name = "t1", files = csvs_i$path, delim = ';', header = T, transaction = T))
  
  # run queries on the R dataframe
  queries_r = map(queries_i_r, ~time_function(eval, list(.)))
  
  # run queries on the duckDB db
  queries_duckDB = map(queries_i_duckDB, ~time_function(eval, list(.)))
  
  size_df = object_size(df)
  size_duckDB = object_size(db)
  
  results_i = tibble_row(
    trials[i, ],
    
    load_time_df = load_time_df, 
    load_time_duckDB = duckDB_load_time,
    
    eval_time_dplyr = queries_r$query_dplyr$time,
    eval_time_duckplyr = queries_r$query_duckplyr$time,
    eval_time_duckDB = queries_duckDB$query_duckDB$time,
    
    result_dplyr = list(queries_r$query_dplyr$object), 
    result_duckplyr = list(queries_r$query_duckplyr$object), 
    result_duckDB = list(queries_duckDB$query_duckDB$object), 
    
    size_df = list(size_df), 
    size_duckDB = list(size_duckDB)
  )
  
  results = bind_rows(results, results_i)
  
  dbDisconnect(db)
}

if (!fs::dir_exists("RDS")) fs::dir_create("RDS")
saveRDS(results, "RDS/results.Rds")
```



## Results

In this section, we examine the results. First, we check whether the results of the queries were consistent.

```{r}
results = readRDS("RDS/results.Rds") |>
  mutate(across(matches("time"),  ~./60)) # convet time to minutes
  
length_check = results |>
  arrange(path, trial) |>
  select(starts_with("result")) |>
  apply(2, unique)
  
# this should be TRUE: it checks whether the df's returned from all the trials are identical
all(map_int(length_check, length) == nrow(csvs)) 

map_int(1:nrow(csvs), function(i) {
  dfs = list()
  for (df in length_check) {
    dfs = append(dfs, df[i])
  }
  length(unique(dfs))
})

# This should be a vector of 1s of length `nrow(csvs)`. It goes line by line and, for each  each csv, returns the number of identical dataframes for each loading procedure. Any number greater than one indicates that there are differences in the loaded dataframes, which is not ideal.
```

An informal inspection of these dataframes indicates that the results are **mostly** the same. The differences are mostly due to situations in which, for example, a value is coded as `NA` in one dataframe and 0 in the other.

This next table gives the mean time for all the events.

```{r}
# means
results |>
  inner_join(csvs) |>
  select(csv, size, matches("time")) |>
  mutate(size = as.character(size)) |>
  group_by(csv, size) |>
  summarise(across(matches("time"), mean)) |>
  mutate(across(matches("time"), ~round(., 1))) |>
  reactable::reactable()
```

This graph shows the load times by file.

```{r}
results |>
  select(csv, trial, load_time_dplyr, load_time_duckDB) |>
  mutate(trial = as_factor(trial)) |>
  pivot_longer(cols = 3:4) |>
  ggplot(aes(fill=name, y=value, x=trial)) +
  facet_wrap(~csv, scales = 'free_y') +
  geom_col(position = 'dodge') +
  ggthemes::theme_clean() +
  theme(legend.position = 'bottom') +
  scale_fill_viridis_d(option = "D") +
  labs(fill=NULL, y="time (min)")
```

This graph gives the load time versus size scatter plot.

```{r}
results |>
  select(csv, size, load_time_dplyr, load_time_duckDB) |>
  pivot_longer(cols = 3:4) |>
  ggplot(aes(x=size, y=value, color=name, shape = csv)) +
  geom_point(size=3) +
  ggthemes::theme_clean() +
  theme(legend.position = 'bottom') +
  labs(color=NULL, x="file size", y="load time (min)") +
  guides(shape = 'none') +
  scale_color_viridis_d(option = "D")
```
